{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---\n",
    "Latex Macros\n",
    "-->\n",
    "$$\n",
    "\\newcommand{\\bar}{\\,|\\,}\n",
    "\\newcommand{\\Xs}{\\mathcal{X}}\n",
    "\\newcommand{\\Ys}{\\mathcal{Y}}\n",
    "\\newcommand{\\y}{\\mathbf{y}}\n",
    "\\newcommand{\\weights}{\\mathbf{w}}\n",
    "\\newcommand{\\balpha}{\\boldsymbol{\\alpha}}\n",
    "\\newcommand{\\bbeta}{\\boldsymbol{\\beta}}\n",
    "\\newcommand{\\aligns}{\\mathbf{a}}\n",
    "\\newcommand{\\align}{a}\n",
    "\\newcommand{\\source}{\\mathbf{s}}\n",
    "\\newcommand{\\target}{\\mathbf{t}}\n",
    "\\newcommand{\\ssource}{s}\n",
    "\\newcommand{\\starget}{t}\n",
    "\\newcommand{\\repr}{\\mathbf{f}}\n",
    "\\newcommand{\\repry}{\\mathbf{g}}\n",
    "\\newcommand{\\x}{\\mathbf{x}}\n",
    "\\newcommand{\\prob}{p}\n",
    "\\newcommand{\\vocab}{V}\n",
    "\\newcommand{\\params}{\\boldsymbol{\\theta}}\n",
    "\\newcommand{\\param}{\\theta}\n",
    "\\DeclareMathOperator{\\perplexity}{PP}\n",
    "\\DeclareMathOperator{\\argmax}{argmax}\n",
    "\\DeclareMathOperator{\\argmin}{argmin}\n",
    "\\newcommand{\\train}{\\mathcal{D}}\n",
    "\\newcommand{\\counts}[2]{\\#_{#1}(#2) }\n",
    "\\newcommand{\\length}[1]{\\text{length}(#1) }\n",
    "\\newcommand{\\indi}{\\mathbb{I}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In the last assignment, you will apply deep learning methods to solve a particular story understanding problem. Automatic understanding of stories is an important task in natural language understanding [[1]](http://anthology.aclweb.org/D/D13/D13-1020.pdf). Specifically, you will develop a model that given a sequence of sentences learns to sort these sentence in order to yield a coherent story [[2]](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/short-commonsense-stories.pdf). This sounds (and to an extent is) trivial for humans, however it is a quite difficult task for machines as it involves commonsense knowledge and temporal understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "You are given a dataset of 45502 instances, each consisting of 5 sentences. Your system needs to ouput a sequence of numbers which represent the predicted order of these sentences. For example, given a story:\n",
    "\n",
    "    He went to the store.\n",
    "    He found a lamp he liked.\n",
    "    He bought the lamp.\n",
    "    Jan decided to get a new lamp.\n",
    "    Jan's lamp broke.\n",
    "\n",
    "your system needs to provide an answer in the following form:\n",
    "\n",
    "    2\t3\t4\t1\t0\n",
    "\n",
    "where the numbers correspond to the zero-based index of each sentence in the correctly ordered story. So \"`2`\" for \"`He went to the store.`\" means that this sentence should come 3rd in the correctly ordered target story. In This particular example, this order of indices corresponds to the following target story:\n",
    "\n",
    "    Jan's lamp broke.\n",
    "    Jan decided to get a new lamp.\n",
    "    He went to the store.\n",
    "    He found a lamp he liked.\n",
    "    He bought the lamp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "To develop your model(s), we provide a training and a development datasets. The test dataset will be held out, and we will use it to evaluate your models. The test set is coming from the same task distribution, and you don't need to expect drastic changes in it.\n",
    "\n",
    "You will use [TensorFlow](https://www.tensorflow.org/) to build a deep learning model for the task. We provide a very crude system which solves the task with a low accuracy, and a set of additional functions you will have to use to save and load the model you create so that we can run it.\n",
    "\n",
    "As we have to run the notebooks of each submission, and as deep learning models take long time to train, your notebook **NEEDS** to conform to the following requirements:\n",
    "* You **NEED** to run your parameter optimisation offline, and provide your final model saved by using the provided function\n",
    "* The maximum size of a zip file you can upload to moodle is 160MB. We will **NOT** allow submissions larger than that.\n",
    "* We do not have time to train your models from scratch! You **NEED** to provide the full code you used for the training of your model, but by all means you **CANNOT** call the training method in the notebook you will send to us.\n",
    "* We will run these notebooks automatically. If your notebook runs the training procedure, in addition to loading the model, and we need to edit your code to stop the training, you will be penalised with **-20 points**.\n",
    "* If you do not provide a pretrained model, and rely on training your model on our machines, you will get **0 points**.\n",
    "* It needs to be tested on the stat-nlp-book Docker setup to ensure that it does not have any dependencies outside of those that we provide. If your submission fails to adhere to this requirement, you will get **0 points**.\n",
    "\n",
    "Running time and memory issues:\n",
    "* We have tested a possible solution on a mid-2014 MacBook Pro, and a few epochs of the model run in less than 3min. Thus it is possible to train a model on the data in reasonable time. However, be aware that you will need to run these models many times over, for a larger number of epochs (more elaborate models, trained on much larger datasets can train for weeks! However, this shouldn't be the case here.). If you find training times too long for your development cycle you can reduce the training set size. Once you have found a good solution you can increase the size again. Caveat: model parameters tuned on a smaller dataset may not be optimal for a larger training set.\n",
    "* In addition to this, as your submission is capped by size, feel free to experiment with different model sizes, numeric values of different precisions, filtering the vocabulary size, downscaling some vectors, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hints\n",
    "\n",
    "A non-exhaustive list of things you might want to give a try:\n",
    "- better tokenization\n",
    "- experiment with pre-trained word representations such as [word2vec](https://code.google.com/archive/p/word2vec/), or [GloVe](http://nlp.stanford.edu/projects/glove/). Be aware that these representations might take a lot of parameters in your model. Be sure you use only the words you expect in the training/dev set and account for OOV words. When saving the model parameters, pre-rained word embeddings can simply be used in the word embedding matrix of your model. As said, make sure that this word embedding matrix does not contain all of word2vec or GloVe. Your submission is limited, and we will not allow uploading nor using the whole representations set (up to 3GB!)\n",
    "- reduced sizes of word representations\n",
    "- bucketing and batching (our implementation is deliberately not a good one!)\n",
    "  - make sure to draw random batches from the data! (we do not provide this in our code!)\n",
    "- better models:\n",
    "  - stacked RNNs (see tf.nn.rnn_cell.MultiRNNCel\n",
    "  - bi-directional RNNs\n",
    "  - attention\n",
    "  - word-by-word attention\n",
    "  - conditional encoding\n",
    "  - get model inspirations from papers on nlp.stanford.edu/projects/snli/\n",
    "  - sequence-to-sequence encoder-decode architecture for producing the right ordering\n",
    "- better training procedure:\n",
    "  - different training algorithms\n",
    "  - dropout on the input and output embeddings (see tf.nn.dropout)\n",
    "  - L2 regularization (see tf.nn.l2_loss)\n",
    "  - gradient clipping (see tf.clip_by_value or tf.clip_by_norm)\n",
    "- model selection:\n",
    "  - early stopping\n",
    "- hyper-parameter optimization (e.g. random search or grid search (expensive!))\n",
    "    - initial learning rate\n",
    "    - dropout probability\n",
    "    - input and output size\n",
    "    - L2 regularization\n",
    "    - gradient clipping value\n",
    "    - batch size\n",
    "    - ...\n",
    "- post-processing\n",
    "  - for incorporating consistency constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Instructions\n",
    "It is important that this file is placed in the **correct directory**. It will not run otherwise. The correct directory is\n",
    "\n",
    "    DIRECTORY_OF_YOUR_BOOK/assignments/2016/assignment3/problem/group_X/\n",
    "    \n",
    "where `DIRECTORY_OF_YOUR_BOOK` is a placeholder for the directory you downloaded the book to, and in `X` in `group_X` contains the number of your group.\n",
    "\n",
    "After you placed it there, **rename the notebook file** to `group_X`.\n",
    "\n",
    "The notebook is pre-set to save models in\n",
    "\n",
    "    DIRECTORY_OF_YOUR_BOOK/assignments/2016/assignment3/problem/group_X/model/\n",
    "\n",
    "Be sure not to tinker with that - we expect your submission to contain a `model` subdirectory with a single saved model! \n",
    "The saving procedure might overwrite the latest save, or not. Make sure you understand what it does, and upload only a single model! (for more details check tf.train.Saver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Instructions\n",
    "This notebook will be used by you to provide your solution, and by us to both assess your solution and enter your marks. It contains three types of sections:\n",
    "\n",
    "1. **Setup** Sections: these sections set up code and resources for assessment. **Do not edit, move nor copy these cells**.\n",
    "2. **Assessment** Sections: these sections are used for both evaluating the output of your code, and for markers to enter their marks. **Do not edit, move, nor copy these cells**.\n",
    "3. **Task** Sections: these sections require your solutions. They may contain stub code, and you are expected to edit this code. For free text answers simply edit the markdown field.  \n",
    "\n",
    "**If you edit, move or copy any of the setup, assessments and mark cells, you will be penalised with -20 points**.\n",
    "\n",
    "Note that you are free to **create additional notebook cells** within a task section. \n",
    "\n",
    "Please **do not share** this assignment nor the dataset publicly, by uploading it online, emailing it to friends etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Instructions\n",
    "\n",
    "To submit your solution:\n",
    "\n",
    "* Make sure that your solution is fully contained in this notebook. Make sure you do not use any additional files other than your saved model.\n",
    "* Make sure that your solution runs linearly from start to end (no execution hops). We will run your notebook in that order.\n",
    "* **Before you submit, make sure your submission is tested on the stat-nlp-book Docker setup to ensure that it does not have any dependencies outside of those that we provide. If your submission fails to adhere to this requirement, you will get 0 points**.\n",
    "* **If running your notebook produces a trivially fixable error that we spot, we will correct it and penalise you with -20 points. Otherwise you will get 0 points for that solution.**\n",
    "* **Rename this notebook to your `group_X`** (where `X` is the number of your group), and adhere to the directory structure requirements, if you have not already done so. ** Failure to do so will result in -1 point.**\n",
    "* Download the notebook in Jupyter via *File -> Download as -> Notebook (.ipynb)*.\n",
    "* Your submission should be a zip file containing the `group_X` directory, containing `group_X.ipynb` notebook, and the `model` directory with _____\n",
    "* Upload that file to the Moodle submission site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'>Setup 1</font>: Load Libraries\n",
    "This cell loads libraries important for evaluation and assessment of your model. **Do not change, move or copy it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:04:56.249298",
     "start_time": "2016-12-20T12:04:54.376398"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "#! SETUP 1 - DO NOT CHANGE, MOVE NOR COPY\n",
    "import sys, os\n",
    "_snlp_book_dir = \"../../../../../\"\n",
    "sys.path.append(_snlp_book_dir)\n",
    "# docker image contains tensorflow 0.10.0rc0. We will support execution of only that version!\n",
    "import statnlpbook.nn as nn\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'>Setup 2</font>: Load Training Data\n",
    "\n",
    "This cell loads the training data. **Do not edit the next cell, nor copy/duplicate it**. Instead refer to the variables in your own code, and slice and dice them as you see fit (but do not change their values). \n",
    "For example, no one stops you from introducing, in the corresponding task section, `my_train` and `my_dev` variables that split the data into different folds.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:04:57.110195",
     "start_time": "2016-12-20T12:04:56.251082"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#! SETUP 2 - DO NOT CHANGE, MOVE NOR COPY\n",
    "data_path = _snlp_book_dir + \"data/nn/\"\n",
    "data_train = nn.load_corpus(data_path + \"train.tsv\")\n",
    "data_dev = nn.load_corpus(data_path + \"dev.tsv\")\n",
    "assert(len(data_train) == 45502)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Structures\n",
    "\n",
    "Notice that the data is loaded from tab-separated files. The files are easy to read, and we provide the loading functions that load it into a simple data structure. Feel free to check details of the loading.\n",
    "\n",
    "The data structure at hand is an array of dictionaries, each containing a `story` and the `order` entry. `story` is a list of strings, and `order` is a list of integer indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:04:57.134033",
     "start_time": "2016-12-20T12:04:57.115270"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'order': [3, 2, 1, 0, 4],\n",
       " 'story': ['His parents understood and decided to make a change.',\n",
       "  'The doctors told his parents it was unhealthy.',\n",
       "  'Dan was overweight as well.',\n",
       "  \"Dan's parents were overweight.\",\n",
       "  'They got themselves and Dan on a diet.']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Task 1</font>: Model implementation\n",
    "\n",
    "Your primary task in this assignment is to implement a model that produces the right order of the sentences in the dataset.\n",
    "\n",
    "### Preprocessing pipeline\n",
    "\n",
    "First, we construct a preprocessing pipeline, in our case `pipeline` function which takes care of:\n",
    "- out-of-vocabulary words\n",
    "- building a vocabulary (on the train set), and applying the same unaltered vocabulary on other sets (dev and test)\n",
    "- making sure that the length of input is the same for the train and dev/test sets (for fixed-sized models)\n",
    "\n",
    "You are free (and encouraged!) to do your own input processing function. Should you experiment with recurrent neural networks, you will find that you will need to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:04:59.842961",
     "start_time": "2016-12-20T12:04:57.136946"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# convert train set to integer IDs\n",
    "train_stories, train_orders, vocab = nn.pipeline(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to make sure that the `pipeline` function returns the necessary data for your computational graph feed - the required inputs in this case, as we will call this function to process your dev and test data. If you do not make sure that the same pipeline applied to the train set is applied to other datasets, your model may not work with that data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:04:59.925263",
     "start_time": "2016-12-20T12:04:59.844598"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# get the length of the longest sentence\n",
    "max_sent_len = train_stories.shape[2]\n",
    "\n",
    "# convert dev set to integer IDs, based on the train vocabulary and max_sent_len\n",
    "dev_stories, dev_orders, _ = nn.pipeline(data_dev, vocab=vocab, max_sent_len_=max_sent_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can take a look at the result of the `pipeline` with the `show_data_instance` function to make sure that your data loaded correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:04:59.954655",
     "start_time": "2016-12-20T12:04:59.926701"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " Story:\n",
      "  The manager decided to offer John the job.\n",
      "  During the interview he was very <OOV> and <OOV>\n",
      "  He went to the interview very prepared and nicely dressed.\n",
      "  John was excited to have a job interview.\n",
      "  The manager of the company was really impressed by John's comments.\n",
      " Order:\n",
      "  [4 2 1 0 3]\n",
      "\n",
      "Desired story:\n",
      "  John was excited to have a job interview.\n",
      "  He went to the interview very prepared and nicely dressed.\n",
      "  During the interview he was very <OOV> and <OOV>\n",
      "  The manager of the company was really impressed by John's comments.\n",
      "  The manager decided to offer John the job.\n"
     ]
    }
   ],
   "source": [
    "nn.show_data_instance(dev_stories, dev_orders, vocab, 155)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions and Custom Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_dataset(x, y, ratio = [0.7, 0.15, 0.15] ):\n",
    "    # number of examples\n",
    "    data_len = len(x)\n",
    "    lens = [ int(data_len*item) for item in ratio ]\n",
    "\n",
    "    trainX, trainY = x[:lens[0]], y[:lens[0]]\n",
    "    testX, testY = x[lens[0]:lens[0]+lens[1]], y[lens[0]:lens[0]+lens[1]]\n",
    "    validX, validY = x[-lens[-1]:], y[-lens[-1]:]\n",
    "\n",
    "    return (trainX,trainY), (testX,testY), (validX,validY)\n",
    "\n",
    "def split_dataset_mlp(x, y, z, ratio = [0.7, 0.15, 0.15] ):\n",
    "    # number of examples\n",
    "    data_len = len(x)\n",
    "    lens = [ int(data_len*item) for item in ratio ]\n",
    "\n",
    "    trainX, trainY, trainZ = x[:lens[0]], y[:lens[0]], z[:lens[0]]\n",
    "    testX, testY, testZ = x[lens[0]:lens[0]+lens[1]], y[lens[0]:lens[0]+lens[1]], z[lens[0]:lens[0]+lens[1]]\n",
    "    validX, validY, validZ = x[-lens[-1]:], y[-lens[-1]:], z[-lens[-1]:]\n",
    "\n",
    "    return (trainX,trainY,trainZ), (testX,testY,testZ), (validX,validY,validZ)\n",
    "\n",
    "def batch_gen(x, y, batch_size):\n",
    "    # infinite while\n",
    "    while True:\n",
    "        for i in range(0, len(x), batch_size):\n",
    "            if (i+1)*batch_size < len(x):\n",
    "                yield x[i : (i+1)*batch_size ].T, y[i : (i+1)*batch_size ].T\n",
    "                \n",
    "def rand_batch_gen(x, y, batch_size):\n",
    "    while True:\n",
    "        sample_idx = sample(list(np.arange(len(x))), batch_size)\n",
    "        yield x[sample_idx].T, y[sample_idx].T\n",
    "        \n",
    "def rand_batch_mlp(x, y, z, batch_size):\n",
    "    while True:\n",
    "        sample_idx = sample(list(np.arange(len(x))), batch_size)\n",
    "        yield np.array(x)[sample_idx], np.array(y)[sample_idx], np.array(z)[sample_idx]\n",
    "        \n",
    "        \n",
    "def decode(sequence, lookup, separator=''): # 0 used for padding, is ignored\n",
    "    return separator.join([ lookup[element] for element in sequence if element ])\n",
    "\n",
    "\n",
    "def getRevVocab(vocab): \n",
    "    return {v: k for k, v in vocab.items()}\n",
    "\n",
    "\n",
    "def flattenStory(stories, lengths): \n",
    "    out_sentences_dev1 = [item for sent in stories for item in sent]\n",
    "    out_seq_len_dev1 = [item for sent in lengths for item in sent]\n",
    "    return out_sentences_dev1, out_seq_len_dev1\n",
    "\n",
    "def getBatchGen(trainX, trainY, batch_size):\n",
    "    counter = 0\n",
    "    while True:\n",
    "        if counter >= shape(trainX)[0] // batch_size:\n",
    "            counter = 0\n",
    "            yield trainX[counter:counter+batch_size].T, trainY[counter:counter+batch_size].T\n",
    "            counter += 1\n",
    "        else: \n",
    "            yield trainX[counter:counter+batch_size].T, trainY[counter:counter+batch_size].T\n",
    "            counter += 1\n",
    "            \n",
    "\n",
    "def getBatchGenMLP(trainX, trainY, trainZ, batch_size):\n",
    "    counter = 0\n",
    "    while True:\n",
    "        if counter >= shape(trainX)[0] // batch_size:\n",
    "            counter = 0\n",
    "            yield trainX[counter:counter+batch_size].T, trainY[counter:counter+batch_size].T, trainZ[counter:counter+batch_size]\n",
    "            counter += 1\n",
    "        else: \n",
    "            yield trainX[counter:counter+batch_size].T, trainY[counter:counter+batch_size].T, trainZ[counter:counter+batch_size]\n",
    "            counter += 1\n",
    "               \n",
    "def orderStories(data, order):\n",
    "    out_sentences_orderd = []\n",
    "    for i, story in enumerate(data): \n",
    "        out_sentences_orderd.append([story[item] for item in order[i]])\n",
    "    return out_sentences_orderd\n",
    "\n",
    "def makeDataSeq2SeqReady(data):\n",
    "    out_sentences_enc = []\n",
    "    out_sentences_dec = []\n",
    "    for i, item in enumerate(data): \n",
    "        out_sentences_enc.append(item[:-1])\n",
    "        out_sentences_dec.append(item[1:])\n",
    "    \n",
    "    return out_sentences_enc, out_sentences_dec\n",
    "\n",
    "def w2vToNumpy():\n",
    "    word2vec = {} #skip information on first line\n",
    "    fin= open('glove.6B.50d.txt')    \n",
    "    for line in fin:\n",
    "        items = line.replace('\\r','').replace('\\n','').split(' ')\n",
    "        if len(items) < 10: continue\n",
    "        word = items[0]\n",
    "        vect = np.array([float(i) for i in items[1:] if len(i) > 1])\n",
    "        word2vec[word] = vect\n",
    "\n",
    "\n",
    "    return word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#my_glove = w2vToNumpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#glove_vocab = {'<PAD>': 0, '<OOV>':1}\n",
    "#for word in my_glove.keys():\n",
    "#    glove_vocab[word] = len(glove_vocab)\n",
    "#glove_embedding = np.zeros((len(glove_vocab), 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Word2Vec():\n",
    "\n",
    "    def __init__(self, raw_data, voc_size, window_size = 2, batch_size = 50, embedding_size = 40, num_sampled = 50):\n",
    "        \n",
    "        self.raw_data = raw_data\n",
    "        self.window_size = window_size \n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.number_neg_samples = num_sampled\n",
    "        self.skip_gram_pairs = self.makeSkipGram(raw_data)\n",
    "        self.trained_embeddings = []\n",
    "        self.voc_size = voc_size\n",
    "        \n",
    "    def makeSkipGram(self, data):    \n",
    "        data = data.flatten()\n",
    "        cbow_pairs = [];\n",
    "        for i in range(1, len(data)-1) :\n",
    "            cbow_pairs.append([[data[i-1], data[i+1]], data[i]]);\n",
    "\n",
    "        skip_gram_pairs = [];\n",
    "        for c in cbow_pairs:\n",
    "            skip_gram_pairs.append([c[1], c[0][0]])\n",
    "            skip_gram_pairs.append([c[1], c[0][1]])\n",
    "            \n",
    "        return skip_gram_pairs\n",
    "        \n",
    "    def generate_batch(self, size):\n",
    "        assert size < len(self.skip_gram_pairs)\n",
    "        x_data=[]\n",
    "        y_data = []\n",
    "        r = np.random.choice(range(len(self.skip_gram_pairs)), size, replace=False)\n",
    "        for i in r:\n",
    "            x_data.append(self.skip_gram_pairs[i][0])  # n dim\n",
    "            y_data.append([self.skip_gram_pairs[i][1]])  # n, 1 dim\n",
    "        return x_data, y_data\n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        train_inputs = tf.placeholder(tf.int32, shape=[self.batch_size])\n",
    "        # need to shape [batch_size, 1] for nn.nce_loss\n",
    "        train_labels = tf.placeholder(tf.int32, shape=[self.batch_size, 1])\n",
    "        # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "        with tf.device('/cpu:0'):\n",
    "            # Look up embeddings for inputs.\n",
    "            embeddings = tf.Variable(\n",
    "                tf.random_uniform([self.voc_size, self.embedding_size], -1.0, 1.0), name = \"emb\")\n",
    "            embed = tf.nn.embedding_lookup(embeddings, train_inputs) # lookup table\n",
    "        \n",
    "        # initialising a saver object that contains the learning embeddings. \n",
    "        \n",
    "        \n",
    "        # Construct the variables for the NCE loss\n",
    "        nce_weights = tf.Variable(\n",
    "            tf.random_uniform([self.voc_size, self.embedding_size],-1.0, 1.0))\n",
    "        nce_biases = tf.Variable(tf.zeros([self.voc_size]))\n",
    "        \n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.nce_loss(nce_weights, nce_biases, embed, train_labels,\n",
    "                         self.number_neg_samples, self.voc_size))\n",
    "        # Use the adam optimizer\n",
    "        train_op = tf.train.AdamOptimizer(1e-1).minimize(loss)\n",
    "        # Initialise saver object\n",
    "        \n",
    "        saver = tf.train.Saver({\"my_embeddings\": embeddings})\n",
    "        \n",
    "        init = tf.initialize_all_variables()\n",
    "        \n",
    "        # Launch the graph in a session\n",
    "        with tf.Session() as sess:\n",
    "            # Initializing all variables\n",
    "            sess.run(init)\n",
    "    \n",
    "            for step in range(30):\n",
    "                batch_inputs, batch_labels = self.generate_batch(self.batch_size)\n",
    "                _, loss_val = sess.run([train_op, loss],\n",
    "                        feed_dict={train_inputs: batch_inputs, train_labels: batch_labels})\n",
    "                if step % 10 == 0:\n",
    "                    print(\"Loss at \", step, loss_val) # Report the loss\n",
    "                # Every 100 steps create checkpoint of current model. \n",
    "                if step %100 == 0: \n",
    "                    print(\"Creating Checkpoint..\")\n",
    "                    #saver.save(sess, SAVER_PATH, global_step = step)\n",
    "            # Final embeddings are ready for you to use. Need to normalize for practical use\n",
    "            #saver.save(sess, SAVER_PATH, global_step = step)\n",
    "            #saver.save(sess, os.path.join(LOG_DIR, \"model.ckpt\"), step)\n",
    "            self.trained_embeddings = embeddings.eval()\n",
    "            #saver.save(sess, \"embedding_checkpoint.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OOV = '<OOV>'\n",
    "PAD = '<PAD>'\n",
    "\n",
    "def ttokenize(scentence):\n",
    "    import re\n",
    "    #word=scentence.split(' ')\n",
    "    word = scentence.lower()\n",
    "    token = re.compile(\"[\\w]+(?=n't)|n't|\\'m|\\'ll|[\\w]+|[.?!;,\\-\\(\\)â€”\\:']\")\n",
    "    t=token.findall(word)\n",
    "    #t=list(reversed(t))\n",
    "    return t\n",
    "\n",
    "def tokenize(input):\n",
    "    print(input.split(' '))\n",
    "    return input.split(' ')\n",
    "\n",
    "def my_pipeline(data, vocab=None, max_sent_len_=None):\n",
    "    is_ext_vocab = True\n",
    "    if vocab is None:\n",
    "        is_ext_vocab = False\n",
    "        vocab = {PAD: 0, OOV: 1}\n",
    "\n",
    "    max_sent_len = -1\n",
    "    data_sentences = []\n",
    "    data_orders = []\n",
    "\n",
    "    out_seq_len = []\n",
    "    \n",
    "    \n",
    "    for instance in data:\n",
    "        sents = []\n",
    "        data_seq_len = []\n",
    "        for sentence in instance['story']:\n",
    "            sent = []\n",
    "            tokenized = ttokenize(sentence)\n",
    "            \n",
    "            data_seq_len.append(len(tokenized))\n",
    "     \n",
    "            for token in tokenized:\n",
    "            \n",
    "                if not is_ext_vocab and token not in vocab:\n",
    "                    vocab[token] = len(vocab)\n",
    "                if token not in vocab:\n",
    "                    token_id = vocab[OOV]\n",
    "                else:\n",
    "                    token_id = vocab[token]\n",
    "                sent.append(token_id)\n",
    "            if len(sent) > max_sent_len:\n",
    "                max_sent_len = len(sent)\n",
    "            sents.append(sent)\n",
    "        \n",
    "        out_seq_len.append(data_seq_len)\n",
    "        \n",
    "        data_sentences.append(sents)\n",
    "        data_orders.append(instance['order'])\n",
    "\n",
    "    if max_sent_len_ is not None:\n",
    "        max_sent_len = max_sent_len_\n",
    "    out_sentences = np.full([len(data_sentences), 5, max_sent_len], vocab[PAD], dtype=np.int32)\n",
    "\n",
    "    for i, elem in enumerate(data_sentences):\n",
    "        for j, sent in enumerate(elem):\n",
    "            out_sentences[i, j, 0:len(sent)] = sent\n",
    "\n",
    "    out_orders = np.array(data_orders, dtype=np.int32)\n",
    "\n",
    "    return out_sentences, out_orders, out_seq_len, vocab, max_sent_len\n",
    "\n",
    "out_sentences, out_orders, out_seq_len, vocab, max_sent_len = my_pipeline(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out_sentences_flat, out_len_flat = flattenStory(out_sentences, out_seq_len)\n",
    "test_stories1, test_orders, test_seq_len1, _ , _= my_pipeline(data_dev, vocab = vocab, max_sent_len_= max_sent_len)\n",
    "test_stories1, test_seq_len1 = flattenStory(test_stories1, test_seq_len1)\n",
    "\n",
    "batch_gen = rand_batch_mlp(out_sentences, out_seq_len, out_orders, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "The model we provide is a rudimentary, non-optimised model that essentially represents every word in a sentence with a fixed vector, sums these vectors up (per sentence) and puts a softmax at the end which aims to guess the order of sentences independently.\n",
    "\n",
    "First we define the model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Embeddings\n",
    "#GloVe_embedings50 = np.load('GloVe_devvocab_emb.npy').item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2seq + Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Seq2SeqOrdering(object):\n",
    "\n",
    "    def __init__(self, xseq_len, yseq_len,\n",
    "            xvocab_size, yvocab_size,\n",
    "            emb_dim, num_layers, ckpt_path,\n",
    "            lr=0.01,\n",
    "            epochs=10, model_name='seq2seq_model'):\n",
    "\n",
    "        # attach these arguments to self\n",
    "        self.xseq_len = xseq_len\n",
    "        self.yseq_len = yseq_len\n",
    "        self.ckpt_path = ckpt_path\n",
    "        self.epochs = epochs\n",
    "        self.model_name = model_name\n",
    "        self.emb_dim = emb_dim\n",
    "        self.epochs = 10000\n",
    "        \n",
    "        \n",
    "        self.mlp_hidden = 64\n",
    "        self.n_classes = 2\n",
    "        self.mlp_input = emb_dim\n",
    "        self.mlp_epochs = 10000\n",
    "\n",
    "\n",
    "        # build thy graph\n",
    "        #  attach any part of the graph that needs to be exposed, to the self\n",
    "        def __graph__():\n",
    "            \n",
    "            \n",
    "            ############### Placeholders for seq2seq ###############\n",
    "\n",
    "            # placeholders\n",
    "            tf.reset_default_graph()\n",
    "            #  encoder inputs : list of indices of length xseq_len\n",
    "            self.enc_ip = [ tf.placeholder(shape=[None,],\n",
    "                            dtype=tf.int64,\n",
    "                            name='ei_{}'.format(t)) for t in range(xseq_len) ]\n",
    "\n",
    "            #  labels that represent the real outputs\n",
    "            self.labels = [ tf.placeholder(shape=[None,],\n",
    "                            dtype=tf.int64,\n",
    "                            name='ei_{}'.format(t)) for t in range(yseq_len) ]\n",
    "\n",
    "            #  decoder inputs : 'GO' + [ y1, y2, ... y_t-1 ]\n",
    "            self.dec_ip = [ tf.zeros_like(self.enc_ip[0], dtype=tf.int64, name='GO') ] + self.labels[:-1]\n",
    "\n",
    "\n",
    "            # Basic LSTM cell wrapped in Dropout Wrapper\n",
    "            self.keep_prob = tf.placeholder(tf.float32)\n",
    "            # define the basic cell\n",
    "            \n",
    "            ############### Set Up LSTM Net ###############\n",
    "\n",
    "            basic_cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "                    tf.nn.rnn_cell.BasicLSTMCell(self.emb_dim, state_is_tuple=True),\n",
    "                    output_keep_prob=self.keep_prob)\n",
    "            # stack cells together : n layered model\n",
    "            stacked_lstm = tf.nn.rnn_cell.MultiRNNCell([basic_cell]*num_layers, state_is_tuple=True)\n",
    "            \n",
    "\n",
    "            # for parameter sharing between training model\n",
    "            #  and testing model\n",
    "            with tf.variable_scope('decoder') as scope:\n",
    "                # build the seq2seq model\n",
    "                #  inputs : encoder, decoder inputs, LSTM cell type, vocabulary sizes, embedding dimensions\n",
    "                self.decode_outputs, self.decode_states = tf.nn.seq2seq.embedding_rnn_seq2seq(self.enc_ip,self.dec_ip, stacked_lstm,\n",
    "                                                    xvocab_size, yvocab_size, emb_dim)\n",
    "                # share parameters\n",
    "                scope.reuse_variables()\n",
    "                # testing model, where output of previous timestep is fed as input\n",
    "                #  to the next timestep\n",
    "                self.decode_outputs_test, self.decode_states_test = tf.nn.seq2seq.embedding_rnn_seq2seq(\n",
    "                    self.enc_ip, self.dec_ip, stacked_lstm, xvocab_size, yvocab_size,emb_dim,\n",
    "                    feed_previous=True)\n",
    "\n",
    "            \n",
    "            ############### Seq2seq Loss ###############\n",
    "            \n",
    "            with tf.variable_scope('loss') as scope: \n",
    "                # weighted loss\n",
    "                #  TODO : add parameter hint\n",
    "                loss_weights = [ tf.ones_like(label, dtype=tf.float32) for label in self.labels ]\n",
    "                self.loss = tf.nn.seq2seq.sequence_loss(self.decode_outputs, self.labels, loss_weights, yvocab_size)\n",
    "                \n",
    "                scope.reuse_variables()\n",
    "                \n",
    "                self.loss_permutation = tf.nn.seq2seq.sequence_loss(self.decode_outputs_test, self.labels, loss_weights, yvocab_size)\n",
    "\n",
    "            \n",
    "            ############### Seq2seq Optimisation ###############\n",
    "            \n",
    "            self.train_op = tf.train.AdamOptimizer(learning_rate=lr).minimize(self.loss)\n",
    "            \n",
    "\n",
    "            self.n_hidden_1 = 512 # 1st layer number of features\n",
    "            self.n_hidden_2 = 256 # 2nd layer number of features\n",
    "            self.n_input = self.emb_dim * 4 \n",
    "            self.n_classes_mlp = 5 \n",
    "            self.learning_rate = 0.01\n",
    "            self.output_size = 25\n",
    "\n",
    "            # tf Graph input\n",
    "            self.x = tf.placeholder(\"float\", [None, self.n_input])\n",
    "            self.y = tf.placeholder(tf.int64, [None, self.n_classes_mlp])\n",
    "\n",
    "\n",
    "            # Store layers weight & bias\n",
    "            self.weights = {\n",
    "                'h1': tf.Variable(tf.random_normal([self.n_input, self.n_hidden_1])),\n",
    "                'h2': tf.Variable(tf.random_normal([self.n_hidden_1, self.n_hidden_2])),\n",
    "                'out': tf.Variable(tf.random_normal([self.n_hidden_2, self.output_size]))\n",
    "            }\n",
    "            self.biases = {\n",
    "                'b1': tf.Variable(tf.random_normal([self.n_hidden_1])),\n",
    "                'b2': tf.Variable(tf.random_normal([self.n_hidden_2])),\n",
    "                'out': tf.Variable(tf.random_normal([self.output_size]))\n",
    "            }\n",
    "\n",
    "            # Construct model\n",
    "            self.logits = self.multilayer_perceptron(self.x, self.weights, self.biases)\n",
    "            #self.y = tf.reshape(self.y, [35, 5])\n",
    "            self.logits_reshaped = tf.reshape(self.logits, [-1, 5, 5])\n",
    "            \n",
    "            self.unpacked_logits = [tensor for tensor in tf.unpack(self.logits_reshaped, axis=1)]\n",
    "            self.softmaxes = [tf.nn.softmax(tensor) for tensor in self.unpacked_logits ]\n",
    "            self.softmaxed_logits = tf.pack(self.softmaxes, axis=1)\n",
    "            self.mlp_predict = tf.arg_max(self.softmaxed_logits , 2)\n",
    "\n",
    "            # Define loss and optimizer\n",
    "            self.mlp_loss = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.y))\n",
    "            self.mlp_optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.mlp_loss)\n",
    "\n",
    "        sys.stdout.write('>> Graph Ready <<')\n",
    "        # build comput graph\n",
    "        __graph__()\n",
    "    \n",
    "    # get the feed dictionary\n",
    "    def get_feed(self, X, Y, keep_prob):\n",
    "        feed_dict = {self.enc_ip[t]: X[t] for t in range(self.xseq_len)}\n",
    "        feed_dict.update({self.labels[t]: Y[t] for t in range(self.yseq_len)})\n",
    "        feed_dict[self.keep_prob] = keep_prob # dropout prob\n",
    "        #print(\">> Made feed dict.\")\n",
    "        return feed_dict\n",
    "    \n",
    "        # Create model\n",
    "    def multilayer_perceptron(self, x, weights, biases):\n",
    "        # Hidden layer with RELU activation\n",
    "        layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "        layer_1 = tf.nn.relu(layer_1)\n",
    "        # Hidden layer with RELU activation\n",
    "        layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "        layer_2 = tf.nn.relu(layer_2)\n",
    "        # Output layer with linear activation\n",
    "        out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "        return out_layer\n",
    "\n",
    "    # run one batch for training\n",
    "    def train_batch(self, sess, train_batch_gen):\n",
    "        # get batches\n",
    "        batchX, batchY = train_batch_gen.__next__()\n",
    "        # build feed\n",
    "        feed_dict = self.get_feed(batchX, batchY, keep_prob=0.5)\n",
    "        _, loss_v = sess.run([self.train_op, self.loss], feed_dict)\n",
    "        return loss_v\n",
    "\n",
    "    def eval_step(self, sess, eval_batch_gen):\n",
    "        # get batches\n",
    "        batchX, batchY = eval_batch_gen.__next__()\n",
    "        # build feed\n",
    "        feed_dict = self.get_feed(batchX, batchY, keep_prob=1.)\n",
    "        loss_v, dec_op_v = sess.run([self.loss, self.decode_outputs_test], feed_dict)\n",
    "        # dec_op_v is a list; also need to transpose 0,1 indices\n",
    "        #  (interchange batch_size and timesteps dimensions\n",
    "        dec_op_v = np.array(dec_op_v).transpose([1,0,2])\n",
    "        return loss_v, dec_op_v, batchX, batchY\n",
    "\n",
    "    # evaluate 'num_batches' batches\n",
    "    def eval_batches(self, sess, eval_batch_gen, num_batches):\n",
    "        losses = []\n",
    "        for i in range(num_batches):\n",
    "            loss_v, dec_op_v, batchX, batchY = self.eval_step(sess, eval_batch_gen)\n",
    "            losses.append(loss_v)\n",
    "        return np.mean(losses)\n",
    "        \n",
    "    # finally the train function that\n",
    "    #  runs the train_op in a session\n",
    "    #   evaluates on valid set periodically\n",
    "    #    prints statistics\n",
    "    def train(self, train_set, valid_set, sess=None ):\n",
    "        # we need to save the model periodically\n",
    "        #saver = tf.train.Saver()\n",
    "        # if no session is given\n",
    "        if not sess:\n",
    "            # create a session\n",
    "            sess = tf.Session()\n",
    "            # init all variables\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        sys.stdout.write('>> Training started <<')\n",
    "        # run M epochs\n",
    "        for i in range(self.epochs):\n",
    "            try:\n",
    "                self.train_batch(sess, train_set)\n",
    "                if i % 100 == 0: #and i% (self.epochs//1) == 0: # TODO : make this tunable by the user\n",
    "                    # save model to disk\n",
    "                    #saver.save(sess, self.ckpt_path + self.model_name + '.ckpt', global_step=i)\n",
    "                    # evaluate to get validation loss\n",
    "                    val_loss = self.eval_batches(sess, valid_set, 16) # TODO : and this\n",
    "                    # print stats\n",
    "                    print('\\nModel saved to disk at iteration #{}'.format(i))\n",
    "                    print('val   loss : {0:.6f}'.format(val_loss))\n",
    "                    sys.stdout.flush()\n",
    "            except KeyboardInterrupt: # this will most definitely happen, so handle it\n",
    "                print('Interrupted by user at iteration {}'.format(i))\n",
    "                self.session = sess\n",
    "                return sess\n",
    "            \n",
    "        return sess\n",
    "    \n",
    "    def trainMLP(self, train_batch, eval_batch, sess): \n",
    "        \n",
    "        for i in range(self.mlp_epochs):\n",
    "            try: \n",
    "                # Get training batch\n",
    "                train_batchX, train_batchY, train_batchZ  = next(train_batch)\n",
    "                \n",
    "                # Determine batch size\n",
    "                batch_size = np.shape(train_batchZ)[0]\n",
    "                # Flatten story for embedding\n",
    "                flatten_enc, flatten_dec = self.flattenBatch(train_batchX.T, train_batchY.T)\n",
    "                # Embedd training batchX\n",
    "                _, embedded_x = self.getSeq2SeqEmbedding(sess, flatten_enc, flatten_dec)\n",
    "                \n",
    "        \n",
    "                final_h = embedded_x[0].h\n",
    "                flatten_embeddings = final_h.reshape(batch_size, 4*self.emb_dim)\n",
    "                feed ={self.x:flatten_embeddings,self.y:array(train_batchZ)}\n",
    "                print(shape(sess.run(self.logits_reshaped, feed_dict=feed)))\n",
    "                if i % 100 == 0:\n",
    "                    train_batchX, train_batchY, train_batchZ  = next(eval_batch)\n",
    "                    flatten_enc, flatten_dec = self.flattenBatch(train_batchX.T, train_batchY.T)\n",
    "                    _, embedded_x = self.getSeq2SeqEmbedding(sess, flatten_enc, flatten_dec)\n",
    "                    final_h = embedded_x[0].h\n",
    "                    flatten_embeddings = final_h.reshape(batch_size, 4*self.emb_dim)\n",
    "                    feed ={self.x:flatten_embeddings,self.y:array(train_batchZ)}\n",
    "                    \n",
    "                    loss, pred = sess.run([self.mlp_loss, self.mlp_predict], feed_dict = feed)\n",
    "                    acc = calculate_accuracy(train_batchZ, pred)\n",
    "                    print(\"Iteration: {} Loss: {} Acc: {}\".format(i, loss, acc))\n",
    "                \n",
    "            except KeyboardInterrupt: \n",
    "                print(\"Training Stopped\")\n",
    "                break\n",
    "        \n",
    "        \n",
    "    \n",
    "    def restore_last_session(self):\n",
    "        saver = tf.train.Saver()\n",
    "        # create a session\n",
    "        sess = tf.Session()\n",
    "        # get checkpoint state\n",
    "        ckpt = tf.train.get_checkpoint_state(self.ckpt_path)\n",
    "        # restore session\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            print(\"Restoring last session at: \", ckpt.model_checkpoint_path)\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            return sess\n",
    "        # return to user\n",
    "        else: \n",
    "            sess.close()\n",
    "            print(\"No session saved.\")\n",
    "    \n",
    "    def getSeq2SeqEmbedding(self, sess, x, y):\n",
    "        feed = self.get_feed(x, y, keep_prob = 1)\n",
    "        dec_out, dec_states = sess.run([self.decode_outputs, self.decode_states], feed_dict = feed)\n",
    "        return dec_out, dec_states\n",
    "    \n",
    "    def flattenBatch(self, x, y): \n",
    "        enc = array([item for something in x for item in something]).T\n",
    "        dec = array([item for something in y for item in something]).T\n",
    "        return enc, dec\n",
    "    \n",
    "    def predictLogits(self, sess, x, y, t): \n",
    "        feed = self.get_feed(x, y, keep_prob = 1)\n",
    "        feed.update({x: t})\n",
    "        return sess.run(self.predict, feed_dict = feed)\n",
    "\n",
    "    # prediction\n",
    "    def predict(self, sess, X):\n",
    "        feed_dict = {self.enc_ip[t]: X[t] for t in range(self.xseq_len)}\n",
    "        feed_dict[self.keep_prob] = 1.\n",
    "        dec_op_v = sess.run(self.decode_outputs_test, feed_dict)\n",
    "        # dec_op_v is a list; also need to transpose 0,1 indices\n",
    "        #  (interchange batch_size and timesteps dimensions\n",
    "        dec_op_v = np.array(dec_op_v).transpose([1,0,2])\n",
    "        # return the index of item with highest probability\n",
    "        return np.argmax(dec_op_v, axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model Simple LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### MODEL PARAMETERS ###\n",
    "\n",
    "## hidden 132, batch 50 - 55.5% \n",
    "target_size = 5\n",
    "vocab_size = len(vocab)\n",
    "input_size = 30\n",
    "n = 5460240\n",
    "hidden_size = 134\n",
    "BATCH_SIZE= 45\n",
    "n_stacks = 1\n",
    "embedding_dim = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then we define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:05:00.995336",
     "start_time": "2016-12-20T12:04:59.968153"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "### Base Line MODEL ###\n",
    "tf.reset_default_graph()\n",
    "## PLACEHOLDERS\n",
    "story = tf.placeholder(tf.int64, [None, max_sent_len], \"story\")        # [batch_size x 5 x max_length]\n",
    "order = tf.placeholder(tf.int64, [None, 5], \"order\")             # [batch_size x 5]\n",
    "sen_len = tf.placeholder(tf.int64, [None], \"sen_len\")\n",
    "batch_size = tf.shape(story)[0]//5\n",
    "\n",
    "W = tf.Variable(tf.constant(0.0, shape=[vocab_size, embedding_dim]),\n",
    "                trainable=False, name=\"W\")\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "\n",
    "# Word embeddings\n",
    "initializer = tf.random_uniform_initializer(-0.1, 0.1)\n",
    "\n",
    "embeddings = tf.get_variable(\"W\", [vocab_size, input_size], initializer=initializer)\n",
    "\n",
    "sentences_embedded = tf.nn.embedding_lookup(embeddings, story)\n",
    "\n",
    "with tf.variable_scope(\"encoder\") as varscope:\n",
    "\n",
    "    basic_cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "                        tf.nn.rnn_cell.BasicLSTMCell(hidden_size, state_is_tuple=True),\n",
    "                        output_keep_prob=keep_prob)\n",
    "\n",
    "    _, final_first = tf.nn.dynamic_rnn(basic_cell, sentences_embedded, sequence_length=sen_len, dtype=tf.float32)\n",
    "\n",
    "    final_firs_h = final_first.h\n",
    "        \n",
    "reshape_final = tf.reshape(final_firs_h, [-1, hidden_size*5])\n",
    "\n",
    "logits_ = tf.contrib.layers.linear(reshape_final, 25)\n",
    "\n",
    "logits = tf.reshape(logits_, [-1, 5, 5])\n",
    "\n",
    "\n",
    "loss = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=order))\n",
    "\n",
    "## Optimizer\n",
    "optim = tf.train.AdamOptimizer(learning_rate)\n",
    "optim_op = optim.minimize(loss)\n",
    "init =tf.initialize_all_variables()\n",
    "\n",
    "unpacked_logits = [tensor for tensor in tf.unpack(logits, axis=1)]\n",
    "softmaxes = [tf.nn.softmax(tensor) for tensor in unpacked_logits]\n",
    "softmaxed_logits = tf.pack(softmaxes, axis=1)#\n",
    "predict = tf.arg_max(softmaxed_logits, 2)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "sess= tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We built our model, together with the loss and the prediction function, all we are left with now is to build an optimiser on the loss:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training \n",
    "\n",
    "We defined the preprocessing pipeline, set the model up, so we can finally train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:05:54.615600",
     "start_time": "2016-12-20T12:05:01.186008"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def trainModel(sess = None):\n",
    "    \n",
    "        if not sess: \n",
    "            sess = tf.Session()\n",
    "            \n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "        for j in range(1):\n",
    "            counter = 0\n",
    "            slow_down = False\n",
    "            slow_slow_down = False\n",
    "            for i in range(n // BATCH_SIZE):\n",
    "                x, y, z = next(batch_gen)\n",
    "                x_flat, y_flat = flattenStory(x, y)\n",
    "                if counter >= len(out_sentences)//BATCH_SIZE - BATCH_SIZE: \n",
    "                    counter =0 \n",
    "                try:\n",
    "                    if slow_down == True and slow_slow_down == False: \n",
    "                        l_r = 0.001\n",
    "                    elif slow_slow_down == True:\n",
    "                        l_r = 0.0001\n",
    "                    else:\n",
    "                        l_r = 0.01\n",
    "                        \n",
    "                    inst_story = x_flat \n",
    "                    inst_order = z\n",
    "                    inst_seq_len = y_flat\n",
    "                    \n",
    "                    feed_dict = {story: inst_story, order: inst_order, sen_len: inst_seq_len, keep_prob:0.5, learning_rate: l_r}\n",
    "                    test = sess.run(optim_op, feed_dict = feed_dict)\n",
    "                   \n",
    "                    if i%10 == 0:\n",
    "                        test_feed_dict = {story:test_stories1 , order: test_orders, sen_len:test_seq_len1,  keep_prob:1.0, learning_rate:l_r}\n",
    "                        test_predicted = sess.run(predict, feed_dict=test_feed_dict)\n",
    "                        test_accuracy = nn.calculate_accuracy(test_orders, test_predicted)\n",
    "                        print('test_accuracy =', test_accuracy)\n",
    "                        if test_accuracy > 0.538 and test_accuracy < 0.55: \n",
    "                            slow_down = True\n",
    "                            slow_slow_down = False\n",
    "                        elif test_accuracy > 0.55: \n",
    "                            slow_down = False\n",
    "                            slow_slow_down = True \n",
    "                        else: \n",
    "                            slow_down = False\n",
    "                            \n",
    "                        if test_accuracy > 0.555:\n",
    "                            nn.save_model(sess)\n",
    "                            print(test_accuracy)\n",
    "                            break\n",
    "\n",
    "                    counter += 1\n",
    "                except KeyboardInterrupt:\n",
    "                    print(\"Training Stopped\")\n",
    "                    nn.save_model(sess)\n",
    "                    break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Error analysis functions from statNLPbook.bio \n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import collections as col\n",
    "import statnlpbook.bio as bio\n",
    "import statnlpbook.util as util\n",
    "\n",
    "def confusion_matrix(dev_predicted, dev_orders):\n",
    "    confusion = col.defaultdict(int)\n",
    "    batch_sz = dev_predicted.shape[0]\n",
    "    for i in range(batch_sz):\n",
    "        for j in range(5):\n",
    "            confusion[(dev_predicted[i][j],dev_orders[i][j])] += 1\n",
    "    return confusion\n",
    "\n",
    "#conf_matrix = confusion_matrix(dev_orders,dev_predicted)\n",
    "#bio.full_evaluation_table(conf_matrix)\n",
    "#util.plot_confusion_matrix_dict(cm_dev,90, outside_label=\"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Assessment 1</font>: Assess Accuracy (50 pts) \n",
    "\n",
    "We assess how well your model performs on an unseen test set. We will look at the accuracy of the predicted sentence order, on sentence level, and will score them as followis:\n",
    "\n",
    "* 0 - 20 pts: 45% <= accuracy < 50%, linear\n",
    "* 20 - 40 pts: 50% <= accuracy < 55\n",
    "* 40 - 70 pts 55 <= accuracy < Best Result, linear\n",
    "\n",
    "The **linear** mapping maps any accuracy value between the lower and upper bound linearly to a score. For example, if your model's accuracy score is $acc=54.5\\%$, then your score is $20 + 20\\frac{acc-50}{55-50}$.\n",
    "\n",
    "The *Best-Result* accuracy is the maximum of the best accuracy the course organiser achieved, and the submitted accuracies scores.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Change the following lines so that they construct the test set in the same way you constructed the dev set in the code above. We will insert the test set instead of the dev set here. **`test_feed_dict` variable must stay named the same**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:05:54.755730",
     "start_time": "2016-12-20T12:05:54.617471"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# LOAD THE DATA\n",
    "data_test = nn.load_corpus(data_path + \"dev.tsv\")\n",
    "# make sure you process this with the same pipeline as you processed your dev set\n",
    "test_stories, test_orders, test_seq_len, _, _ = my_pipeline(data_test, vocab=vocab, max_sent_len_=max_sent_len)\n",
    "test_stories, test_seq_len = flattenStory(test_stories, test_seq_len)\n",
    "# THIS VARIABLE MUST BE NAMED `test_feed_dict`\n",
    "test_feed_dict = {story: test_stories, order: test_orders, sen_len: test_seq_len}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code loads your model, computes accuracy, and exports the result. **DO NOT** change this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:05:55.116609",
     "start_time": "2016-12-20T12:05:54.758571"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55521111704970605"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#! ASSESSMENT 1 - DO NOT CHANGE, MOVE NOR COPY\n",
    "with tf.Session() as sess:\n",
    "    # LOAD THE MODEL\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, './model/model.checkpoint')\n",
    "    \n",
    "    # RUN TEST SET EVALUATION\n",
    "    dev_predicted = sess.run(predict, feed_dict=test_feed_dict)\n",
    "    dev_accuracy = nn.calculate_accuracy(dev_orders, dev_predicted)\n",
    "\n",
    "dev_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='orange'>Mark</font>:  Your solution to Task 1 is marked with ** __ points**. \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Task 2</font>: Describe your Approach\n",
    "\n",
    "Enter a 750 words max description of your approach **in this cell**.\n",
    "Make sure to provide:\n",
    "- an **error analysis** of the types of errors your system makes\n",
    "- compare your system with the model we provide, focus on differences and draw useful comparations between them\n",
    "\n",
    "Should you need to include figures in your report, make sure they are Python-generated. For that, feel free to create new cells after this cell (before Assessment 2 cell). Link online images at your risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach summary\n",
    "The presented model (section 1.3.2) is a dynamic recurrent neural network, constructed in TensorFlow, which takes vectorised, concatenated sentances and passes each story through a LSTM cell with a drop-out wrapper, applies a linear layer to the output, to produce logits which are softmaxed and argmaxed to predict a class for each sentence. The model trains in under 15 minutes and reaches a test accuracy of 55.5%.\n",
    "\n",
    "We also built a state-of-the-art 'sequence to sequence' model combined with a Multi-layered Perceptron (section 1.3.1) following Logeswaran et al. (2017) but this failed to beat the test accuracy of the LSTM model (reaching only 11%) for this dataset. We have included this model as demonstration of our efforts. \n",
    "\n",
    "### 2.3. Pre-processing\n",
    "#### 2.3.1 Pipeline enhancement\n",
    "We  adjusted the provided pipeline function to improve the pre-processing and provide additional information required for our more complex model. Such as, improving tokenisation, adding sentence length and other helper functions. \n",
    "\n",
    "#### 2.3.1 Word embedding\n",
    "We built and tested word embeddingâ€™s with Google's 'word2vec' and Stanford's 'Glove' techniques (TensorFlow (2017) and Pennington et al. (2014) respectively). We trained the models and tested each model at 100, 200 and 300 dimensions and we implemented the Wiki 2014 pre-trained word vectors from GloVe (http://www-nlp.stanford.edu/projects/glove/#discuss). \n",
    "\n",
    "However, word embeddings from neither pre-trained or corpus train improved either models accuracy compared to a baseline of random uniform variables as shown below.\n",
    "\n",
    "<table style=\"width:80%\">\n",
    "  <tr>\n",
    "    <th>Algorithm</th>\n",
    "    <th>Dimension</th>\n",
    "    <th>Perfomance compared to baseline</th>  \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>GloVe corpus trained</b></td>\n",
    "    <td>300D</td> \n",
    "    <td>-7.5%</td> \n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>GloVe pre-trained</td>\n",
    "    <td>300D</td> \n",
    "    <td>-5.0%</td> \n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>GloVe corpus trained</td>\n",
    "    <td>100D</td> \n",
    "    <td>-3.5%</td> \n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>word2vec corpus trained</td>\n",
    "    <td>100D</td> \n",
    "    <td>-5.0%</td> \n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "As a result of this findings, we used the same random uniform embeddings as the stub model. We optimised our model for the number of dimensions and found 40 dimensions to provide the best performance. \n",
    "\n",
    "### 2.4. Models\n",
    "We built two different models as shown in section 1.3.1. 'Sequence-to-sequence + Multi-layered Perceptron' and 1.3.2 'LSTM model'. The structure of each model is described below. \n",
    "\n",
    "#### 2.2.1 Sequence-to-sequence + Multi-layered Perceptron\n",
    "The most promising in theory and by far the most complex model we have built is the **Seq2SeqOrdering** class. This model is based upon the idea of coherence modelling through sequence to sequence prediction. Hence, if a machine can predict the most probable subsequent sentence given the current sentence maximizing the log likelihood of the form\n",
    "\n",
    "$$ L(s_{i}, s_{i+1}) = \\frac{1}{N_{i}}\\log p (s_{i+1} | s_{i})$$\n",
    "\n",
    "would allow to discriminate between coherent and arbitrary sentence structures. \n",
    "\n",
    "This model is trained in two stages: \n",
    "\n",
    "> â€¢Â training a seq2seq model using tensorflow's embedding_rnn_seq2seq wrapper by feeding it the sentences in the right order. \n",
    "\n",
    "> â€¢Â training a 2-layer perceptron that scores every permutation (120) of the story.\n",
    "\n",
    "The multi-layer perceptron is trained on the embedding producing by the seq2seq model when an unordered sentence pair is passed into the encoder/decoder. Although the model is efficient to train, producing good sentence predictions after only a few epoch, the inefficiency of the model comes from the fact that it tests each story for each permutation. Despite its good predictive performance, with a mean prediction time of ~10s this model becomes, at least for this assignment, computational intractable. \n",
    "\n",
    "#### 2.2.2 LSTM model \n",
    "Our baseline model consists of a dynamic RNN with a single LSTM cell. Every sentence is fed into the RNN individually creating a sentence embedding. The five-sentence embeddingâ€™s of a story are concatenated and passed into a single linear neural network layer that scores the embeddingâ€™s using a softmax. \n",
    "\n",
    "To prevent overfitting and dead neurons the LSTM cell is wrapped into a dropout container that randomly turns neurons on and off. By doing this we could take advantage of an increased number of hidden layers. \n",
    "\n",
    "While increasing the number of hidden layers only marginally improved the final result the model convergered very quickly to its maximum accuracy rate. \n",
    "\n",
    "An adaptive learning rate was incorporated into the model training to reduce the learning rate as test accuracy reached above 53%. This substantially slowed the optimisation rate down at latter stages of training to reduce voliatity and find a better optimum. \n",
    "\n",
    "The LSTM model is far more computationally advance than the stub model that doesnâ€™t use a NN. NNs are able to learn very complex patterns by having very high degrees of freedom compared to linear or simpler models. The LSTM cell benefits further by being able to store and pass memory sequentially which gives it good prediction power for sequential tasks like this. \n",
    "\n",
    "### 2.3 Parameter optimisation\n",
    "Parameters optimisation was a substantial activity as both models contained a large number of parameters to be tuned. We performed mesh-grid searches on key parameters such as learning rate, batch size and hidden layer size and we're about to add around 15% accuracy by finding local minimas.\n",
    "\n",
    "The key parameters for the optimisation were: learning rate, batch size, hidden layer size and embedding size.\n",
    "\n",
    "This process had not been done for the stub model and with quick optimisation of the learning rate we were able to improve the stubs performance to 40%. The stub doesnâ€™t have many other parameters which reduce to amount of optimiation required. \n",
    "\n",
    "### 2.4. Model performance\n",
    "<table style=\"width:80%\">\n",
    "  <tr>\n",
    "    <th>Model</th>\n",
    "    <th>Description</th>\n",
    "    <th>Accuracy</th>  \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>LSTM model</b></td>\n",
    "    <td>Our model using a single LSTM cell to encode each sentence of a story individually.</td> \n",
    "    <td>55.5% Baseline</td> \n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td><b>Stacked LSTM</b></td>\n",
    "    <td>Added a second LSTM cell to the dynamic RNN. Introducing another cell had similar results as adding more \n",
    "    hidden layers to the cell itself. The results did only marginally imporved at a cost of substantially elevated training times.</td> \n",
    "    <td>54.2%</td> \n",
    "  </tr>\n",
    "    </tr>\n",
    "     <tr>\n",
    "    <td><b>Stub</b></td>\n",
    "    <td>Simple linear model with embeddings</td> \n",
    "    <td>36%</td>\n",
    "   <tr>\n",
    "    <td><b>Sequence-to-sequence + Multi-layered Perceptron</b></td>\n",
    "    <td>Contains the 5-word history preceding the trigger\n",
    "    word. If the trigger word has a history that is \n",
    "    shorter than 5 words, a [Start] token is inserted.</td> \n",
    "    <td>11.33%</td>\n",
    "\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "### 2.6. Error analysis\n",
    "\n",
    "#### 2.6.1 Confusion matrix\n",
    "It is shown from the confusion matrix that the first sentence is the most accurately predicted (85%), followed by the second and the last sentences (50-53%). Sentence 3 and 4 are the least accurately predicted (30% each).\n",
    "\n",
    "The first sentences high accuracy is likely to be caused by it having a the most distinct sentence semantics. The first sentence sets the context for the rest of the story and thus may be more distinguishable. For example, the first sentence is restricted in identifying a person by name whereas other sentences can use 'he','she','him', ect.\n",
    "\n",
    "Sentence 3 and 4 are in the middle of the story and as such are likely to be the most interchangeable as the wide-ranging story topics are determined here. This will result in a weaker pattern for the NN to identify and lower accuracy.   \n",
    "\n",
    "#### 2.6.2 Precision, Recall and F1\n",
    "The model precision and recall agree with the above trend with showing a high recall accuracy (88%) for sentence 1 and lowest accuracy for sentences 2 and 3 (30%).\n",
    "\n",
    "\n",
    "### 2.7. Further improvements\n",
    "- Improved sequence-to-sequence modelling\n",
    "- Mulit-layered LSTMs - Additional layers of LSMT cells enable more patterns to be learnt form the data. We stacked LSTM cells using the TensorFlow (using rnn_cell.MultiRNNCell) however it worsened performance. Further work could look at optimising the implementation.\n",
    "- Use third party CPUs to perform grid search on all parameters\n",
    "\n",
    "### Appendix A. References\n",
    "Lajanugen Logeswaran, Honglak Lee & Dragomir Radev. 2017. Sentance ordering using Recurrent Neural Networks \n",
    "\n",
    "Jeffrey Pennington, Richard Socher, Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation (http://www-nlp.stanford.edu/pubs/glove.pdf)\n",
    "\n",
    "TensorFlow. 2017. Vector Representations of Words (https://www.tensorflow.org/tutorials/word2vec/)\n",
    "\n",
    "Diederik P. Kingma and Jimmy Lei Ba. 2015. Adam: A method for stochastic optimisation  (https://arxiv.org/pdf/1412.6980.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Assessment 2</font>: Assess Description (30 pts) \n",
    "\n",
    "We will mark the description along the following dimensions: \n",
    "\n",
    "* Clarity (10pts: very clear, 0pts: we can't figure out what you did, or you did nothing)\n",
    "* Creativity (10pts: we could not have come up with this, 0pts: Use only the provided model)\n",
    "* Substance (10pts: implemented complex state-of-the-art classifier, compared it to a simpler model, 0pts: Only use what is already there)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='orange'>Mark</font>:  Your solution to Task 2 is marked with ** __ points**.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='orange'>Final mark</font>: Your solution to Assignment 3 is marked with ** __points**. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
